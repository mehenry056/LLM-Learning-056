import gradio as gr
import os
from openai import OpenAI

# å¸¸é‡å®šä¹‰
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
OPENAI_BASE_URL = "https://platform.openai.com/docs"
GROQ_BASE_URL = "https://api.groq.com/openai/v1"
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta"   # Google Geminiï¼ˆæ³¨æ„ï¼šä¸æ˜¯å®Œå…¨ OpenAI å…¼å®¹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†æˆ–ç”¨å®˜æ–¹ SDKï¼‰
GROK_BASE_URL = "https://api.x.ai/v1"                        # xAI Grok å®˜æ–¹ï¼ˆå®Œå…¨å…¼å®¹ OpenAI SDKï¼‰

# æ¨¡å‹æ˜ å°„è¡¨
MODEL_MAP = {
    "openai": [
        "gpt-4o", "gpt-4o-mini", "gpt-4-turbo",
        "gpt-3.5-turbo", "gpt-3.5-turbo-16k"
    ],
    "groq": [
        "llama-3.1-70b-versatile", "llama-3.1-8b-instant",
        "llama3-70b-8192", "llama3-8b-8192",
        "mixtral-8x7b-32768", "gemma-7b-it"
    ],
    "dashscope": [
        "qwen-max", "qwen-plus", "qwen-turbo",
        "qwen-long", "qwen-vl-max", "qwen-vl-plus"
    ],
    "gemini": [                               # Google Gemini ç³»åˆ—
        "gemini-1.5-pro",                     # ç›®å‰æœ€å¼ºï¼Œæ¨èä¸»åŠ›ä½¿ç”¨
        "gemini-1.5-flash",                   # é€Ÿåº¦å¿«ã€æ€§ä»·æ¯”é«˜
        "gemini-1.0-pro",                     # æ—§ç‰ˆç¨³å®šç‰ˆï¼ˆéƒ¨åˆ†åœºæ™¯ä»å¯ç”¨ï¼‰
        "gemini-pro-vision"                   # å¤šæ¨¡æ€ï¼ˆæ”¯æŒå›¾ç‰‡ï¼‰
    ],
    "grok": [                                 # xAI Grok ç³»åˆ—
        "grok-4",                             # æœ€æ–°æœ€å¼ºæ¨¡å‹ï¼ˆé€æ­¥å¼€æ”¾ä¸­ï¼‰
        "grok-3",                             # é«˜æ€§èƒ½ä¸»åŠ›æ¨¡å‹
        "grok-3-mini",                        # è½»é‡å¿«é€Ÿç‰ˆ
        "grok-2",                             # æ—©æœŸç‰ˆæœ¬ï¼ˆå…¼å®¹æ€§å¥½ï¼‰
        "grok-2-vision"                       # æ”¯æŒå›¾åƒçš„å¤šæ¨¡æ€ç‰ˆæœ¬
    ]
}


def update_model_list(provider_choice):
    provider_key = provider_choice.lower()
    models = MODEL_MAP.get(provider_key, MODEL_MAP["dashscope"])
    return gr.Dropdown(choices=models, value=models[0], label="æ¨¡å‹")


def clear_chat():
    return None, []


def chat_with_llm(message, history, provider, model, temperature):
    if not message or not message.strip():
        yield "", history
        return

    # 1. è¿™é‡Œçš„ history ç°åœ¨æ˜¯ä¸€ä¸ª list of dicts: [{'role': 'user', 'content': '...'}, ...]
    provider = provider.lower()
    api_key_env = {
        "openai": "OPENAI_API_KEY",
        "groq": "GROQ_API_KEY",
        "dashscope": "DASHSCOPE_API_KEY",
        "gemini": "GEMINI_API_KEY",  # æˆ– GOOGLE_API_KEYï¼ˆGoogle å®˜æ–¹æ¨èï¼‰
        "grok": "XAI_API_KEY"  # xAI å®˜æ–¹ä½¿ç”¨çš„ç¯å¢ƒå˜é‡å
    }.get(provider)

    api_key = os.getenv(api_key_env)

    if not api_key:
        error_msg = f"âš ï¸ æœªæ£€æµ‹åˆ° {provider.upper()} çš„ API Keyï¼"
        # å…¼å®¹æ–°æ ¼å¼ï¼šæ·»åŠ ç”¨æˆ·æ¶ˆæ¯å’Œé”™è¯¯æç¤º
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})
        yield "", history
        return

    try:
        # å®¢æˆ·ç«¯åˆå§‹åŒ–ä¿æŒä¸å˜
        if provider == "dashscope":
            client = OpenAI(api_key=api_key, base_url=DASHSCOPE_BASE_URL)
            extra_body = {"enable_search": True}
        elif provider == "groq":
            client = OpenAI(api_key=api_key, base_url=GROQ_BASE_URL)
            extra_body = {}
        elif provider == "gemini":
            client = OpenAI(api_key=api_key, base_url=GEMINI_BASE_URL)
            extra_body = {}
        elif provider == "grok":
            client = OpenAI(api_key=api_key, base_url=GROK_BASE_URL)
            extra_body = {}
        else:
            client = OpenAI(api_key=api_key)
            extra_body = {}

        # 2. æ„é€ å‘é€ç»™ API çš„ messagesï¼ˆç°åœ¨ history å·²ç»æ˜¯è¿™ä¸ªæ ¼å¼äº†ï¼Œå¯ä»¥ç›´æ¥è¿½åŠ ï¼‰
        messages = history + [{"role": "user", "content": message}]

        # 3. æ›´æ–° UIï¼šå…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼Œå¹¶é¢„ç•™ä¸€ä¸ªç©ºçš„åŠ©æ‰‹å›å¤ä½
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        yield "", history

        full_response = ""
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,
            extra_body=extra_body
        )

        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                # æ›´æ–°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯çš„å†…å®¹
                history[-1]["content"] = full_response
                yield "", history

        if not full_response.strip():
            history[-1]["content"] = "ï¼ˆæ¨¡å‹æœªè¿”å›æœ‰æ•ˆå†…å®¹ï¼‰"
            yield "", history

    except Exception as e:
        error_msg = f"âŒ è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
        history[-1]["content"] = error_msg
        yield "", history


# Gradio 6.0+ å…¼å®¹å†™æ³•
with gr.Blocks() as demo:  # ç§»é™¤äº† theme å’Œ title å‚æ•°
    gr.Markdown("""
    # ğŸ¤– ğŸå¤šæ¨¡å‹ AI èŠå¤©åŠ©æ‰‹

    æ”¯æŒ OpenAIã€Groqã€é€šä¹‰åƒé—®ï¼ˆDashScopeï¼‰ç­‰å¤šç§æ¨¡å‹ä¸€é”®åˆ‡æ¢  
    è¯·æå‰åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®å¯¹åº”çš„ API Key
    """)

    with gr.Row():
        with gr.Column(scale=4):
            # ç§»é™¤ type="tuples" å‚æ•°ï¼ˆæ–°ç‰ˆé»˜è®¤å°±æ˜¯ tuplesï¼‰
            chatbot = gr.Chatbot(
                height=600,
                show_label=False,
                avatar_images=(None, "https://avatars.githubusercontent.com/u/148468537?s=200&v=4")
            )
            msg = gr.Textbox(
                label="è¾“å…¥ä½ çš„é—®é¢˜",
                placeholder="åœ¨è¿™é‡Œè¾“å…¥æ¶ˆæ¯ï¼Œç„¶åæŒ‰å›è½¦æˆ–ç‚¹å‡»å‘é€...",
                lines=3
            )

            with gr.Row():
                submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", scale=2)
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")

        with gr.Column(scale=1, min_width=300):
            gr.Markdown("### âš™ï¸ é…ç½®")
            provider = gr.Dropdown(
                choices=list(MODEL_MAP.keys()),
                value="dashscope",
                label="æ¨¡å‹æä¾›å•†",
                info="é€‰æ‹© API æœåŠ¡å•†"
            )
            model = gr.Dropdown(
                choices=MODEL_MAP["dashscope"],
                value="qwen-max",
                label="æ¨¡å‹"
            )
            temperature = gr.Slider(
                minimum=0,
                maximum=1.5,
                value=0.7,
                step=0.1,
                label="æ¸©åº¦ (Temperature)",
                info="å€¼è¶Šé«˜è¶Šæœ‰åˆ›é€ æ€§ï¼Œè¶Šä½è¶Šç¡®å®šæ€§"
            )

            gr.Markdown("### â„¹ï¸ ä½¿ç”¨æç¤º")
            gr.Markdown("""
            - OpenAI â†’ `OPENAI_API_KEY`
            - Groq â†’ `GROQ_API_KEY`  
            - é€šä¹‰åƒé—® â†’ `DASHSCOPE_API_KEY`
            """)

    # äº‹ä»¶ç»‘å®š
    provider.change(
        fn=update_model_list,
        inputs=provider,
        outputs=model
    )

    msg.submit(
        fn=chat_with_llm,
        inputs=[msg, chatbot, provider, model, temperature],
        outputs=[msg, chatbot]
    )
    submit_btn.click(
        fn=chat_with_llm,
        inputs=[msg, chatbot, provider, model, temperature],
        outputs=[msg, chatbot]
    )

    clear_btn.click(
        fn=clear_chat,
        outputs=[chatbot, msg]
    )

if __name__ == "__main__":
    demo.queue(max_size=20).launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,          # å¦‚æœéœ€è¦å…¬ç½‘è®¿é—®ï¼Œå¯ä»¥æ”¹ä¸º True
        inbrowser=True,       # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        theme=gr.themes.Soft()  # ç¾åŒ–ä¸»é¢˜ï¼ˆæ¨èä¿ç•™ï¼‰
    )